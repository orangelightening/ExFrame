# BrainUse: Evidence-Based Hiring Intelligence Platform

**Tagline:** *Don't hire a resume. Hire a brain.*

**Vision:** Transform hiring from subjective guesswork into objective brain assessment, saving businesses $200B annually in bad hire costs.

---

## Executive Summary

### The Problem

**Cost of Bad Hires:** $200 billion/year to U.S. businesses alone
- 46% of new hires fail within 18 months (Harvard Business Review)
- Average cost per bad hire: $240,000 (leadership roles)
- Traditional hiring tools (resumes, interviews, references) measure *credentials*, not *capability*

**Why Traditional Hiring Fails:**
- **Resumes** show what they claim, not what they can do
- **Interviews** reveal rehearsed answers under artificial stress
- **References** are vague and liability-averse
- **Take-home tests** are gameable and don't reflect natural work
- **None measure the most critical factor:** *learning ability*

### The Solution

**BrainUse** uses ExFrame's **Tao (Knowledge Cartography)** system to observe candidates' natural learning behavior over 10 days across 3 carefully-chosen domains.

**What We Measure:**
1. **Learning Velocity** - How fast they progress from novice (L1) to expert (L4)
2. **Question Sophistication** - Depth and complexity of inquiry
3. **Disciplined Learning** - Performance on boring vs. interesting material
4. **Cross-Domain Integration** - Ability to connect concepts across fields
5. **Persistence** - Chain depth (how far they follow inquiry threads)
6. **Curiosity Patterns** - What naturally interests them vs. what doesn't

**Key Insight from Medical Schools:**
Universities with medical programs rate **concert pianists** highly for admission because they've proven the ability to:
- Expand procedural memory (10,000+ hours of practice)
- Master complex systems (music theory, performance technique)
- Maintain discipline over years (daily practice routines)
- Perform under pressure (live concerts, competitions)

**BrainUse reveals the same capacities** in 10 days through observable learning behavior.

---

## The Concert Pianist Principle

### Why Medical Schools Love Musicians

Medical schools discovered that concert-level musicians have exceptional success rates in medical programs because they demonstrate:

| Musician Trait | Medical School Requirement | BrainUse Measurement |
|----------------|---------------------------|----------------------|
| Memory expansion (10,000+ hours) | Memorize anatomy, pharmacology | Learning velocity (L1→L4) |
| Complex system mastery | Understand interconnected body systems | Cross-domain analogies |
| Disciplined practice | Study boring material consistently | Interest ratio (boring vs. interesting) |
| Performance under pressure | High-stakes patient decisions | Persistence under ambiguity |
| Pattern recognition | Diagnose from symptoms | Question sophistication |

### The BrainUse Equivalent

We're testing for the **same underlying capacities** that predict success:

**Not:** "Do you have a CS degree?"
**But:** "Can you learn complex systems rapidly like a concert pianist learns symphonies?"

**Not:** "Can you code Python?"
**But:** "Can you progress from 'what is Python?' to 'how does GIL affect threading?' in 6 days?"

**Not:** "Are you motivated?"
**But:** "Do you maintain learning discipline even on boring material?"

This is **capability assessment**, not credential verification.

---

## Detailed Workflow

### Phase 1: Pre-Assessment (Day 0)

**1.1 Knowledge Assessment Survey**

Candidate completes 30-minute online assessment:
- 60 multiple-choice questions across 8-10 domains
- Identifies domains where they're **novices** (< 20% correct)
- System suggests 6-8 domains for learning assessment

**Why this matters:**
- Can't measure learning on topics they already know
- Need genuine learning, not knowledge display
- Ensures fair baseline across all candidates

**1.2 Interest Ranking**

Candidate ranks the 6-8 novice domains from most → least interesting:

```
┌─────────────────────────────────────────────┐
│ Drag these into order from MOST to LEAST   │
│ interesting to you personally:              │
│                                              │
│  ☐ Machine Learning                         │
│  ☐ Cloud Architecture                       │
│  ☐ Database Design                          │
│  ☐ Team Leadership                          │
│  ☐ Compliance & Security                    │
│  ☐ Performance Optimization                 │
│  ☐ Technical Documentation                  │
│  ☐ API Design                               │
└─────────────────────────────────────────────┘
```

**System assigns 3 domains:**
- 1 from top 2 (most interesting)
- 1 from middle (neutral)
- 1 from bottom 2 (least interesting)

**Why this matters:**
- Measures **disciplined learning** (do they learn when bored?)
- Reveals **Interest Ratio** (performance drop on uninteresting work)
- Predicts **job fit** (some roles require boring work tolerance)

**1.3 Candidate Onboarding**

Email with:
- ExFrame login credentials
- 3 assigned domains (labeled A, B, C - interest level hidden)
- Instructions: *"Explore these domains naturally. Ask any questions. We're observing how you learn, not testing what you know."*
- Duration: 10 days
- No other instructions (natural behavior observation)

### Phase 2: Learning Period (Days 1-10)

**What Happens:**
- Candidate uses ExFrame naturally to explore assigned domains
- Every query/response is automatically stored by Tao
- System captures:
  - Query text and timestamp
  - Question sophistication level (L1-L4, auto-classified)
  - Parent query ID (which question led to this one)
  - Response quality and patterns used
  - Domain context

**Candidate Experience:**
- Ask questions in natural language
- Get AI-powered answers (Librarian persona with document search)
- No time pressure, no surveillance feel
- Optional: Can use ** prefix for deeper semantic search
- Optional: Can ask follow-up questions

**System Behavior (Silent):**
- Auto-classifies every question (L1, L2, L3, L4)
- Tracks query chains (parent_query_id)
- Measures time gaps between queries
- Detects cross-domain analogies
- Calculates learning velocity in real-time
- Flags gaming behavior (duplicate questions, etc.)

**What We're Observing:**

| Metric | What It Reveals | Example |
|--------|----------------|---------|
| **Learning Velocity** | Speed of progression L1→L4 | 0.41 = Exceptional (Top 3%) |
| **Question Depth** | Sophistication over time | L1→L2→L3→L4 in 6 days |
| **Interest Ratio** | Discipline vs. motivation | 2.65 = Only learns when motivated |
| **Chain Depth** | Persistence on topics | 5.2 queries deep = Very persistent |
| **Cross-Domain Analogies** | Systems thinking | 12 connections between domains |
| **Curiosity Index** | Natural engagement | 137 = Top 2% |

**Red Flags Auto-Detected:**
- Gaming behavior (repeating questions)
- Surface-only questions (no L3/L4 ever reached)
- Drop-off (engagement decay over time)
- Uniform performance (suspiciously similar across all domains)

### Phase 3: Report Generation (Day 11)

**Automated Analysis:**

Tao system generates comprehensive **Candidate Assessment Report** including:

**3.1 Executive Summary**
- Overall rating (Exceptional / Excellent / Good / Average / Below Average)
- Top strengths (3-5 bullet points)
- Growth areas (2-3 bullet points)
- Hiring recommendation (Strong Hire / Hire / Maybe / Pass)

**3.2 Domain Performance Analysis**
- Per-domain breakdown with metrics
- Learning velocity, depth, question count
- Sophistication progression (L1→L2→L3→L4 timeline)
- Tao Index per domain

**3.3 Interest Ratio Analysis**
- Performance on interesting vs. boring domains
- Interest ratio calculation
- Job fit recommendations based on ratio
- Management implications

**3.4 Cross-Domain Synthesis**
- Analogies discovered (with similarity scores)
- Evidence of systems thinking
- Integration capability assessment

**3.5 Learning Profile Summary**
- Velocity, depth, curiosity, integration scores
- Percentile rankings (vs. benchmark database)
- Tao Index trend graph over 10 days
- Comparison to typical candidates

**3.6 Red Flags Check**
- Any gaming behavior detected
- Engagement consistency
- Domain coverage
- Authenticity assessment

**3.7 Benchmarking**
- Comparison to "typical senior engineer" baseline
- Comparison to role-specific benchmarks
- Percentile rankings on all metrics

**3.8 Interview Question Suggestions**
- Questions derived from actual behavior
- "You asked about [specific topic]. Tell me more..."
- Behavioral evidence-based interview prep

### Phase 4: Human Review & Interview (Days 12-15)

**4.1 HR Review**

Hiring manager receives:
- PDF report (confidential)
- Dashboard view (if multiple candidates)
- Rank-ordered candidate comparison
- Interview question suggestions

**4.2 Strategic Interview**

Traditional interview, but informed by data:
- Ask about *actual questions they asked*
- Probe *actual analogies they made*
- Understand *actual learning patterns observed*
- Validate report findings through dialogue

**Example questions:**
- "You progressed from basic Python to GIL implementation in 6 days. How did you approach that learning?"
- "You connected async patterns to event-driven architecture. Tell me about that insight."
- "Your performance dropped on compliance topics. How do you stay engaged with tedious work?"

### Phase 5: Decision & Offer (Days 16-20)

**Hiring Decision:**
- Combine Tao report (objective data) + interview (subjective validation)
- Compare to other candidates using dashboard
- Make evidence-based hire/no-hire decision

**Offer Strategy:**
- Tao data informs comp level (top 3% learners get top-tier offers)
- Tao data informs role assignment (interest ratio → role fit)
- Tao data informs onboarding (we know their learning style)

---

## Technical Requirements

### ExFrame + Tao Stack

**Required Components:**

1. **ExFrame Core** (Already Built)
   - Domain management
   - Pattern storage and retrieval
   - LLM integration (OpenAI/Anthropic/etc.)
   - Persona system (Librarian for hiring)
   - Query/response processing

2. **Tao Subsystem** (Already Built)
   - Storage: Compressed query history (query_history.json.gz)
   - Analysis: Sessions, chains, relations, concepts, depth, sophistication
   - API: 11 REST endpoints
   - Frontend: Web UI for viewing analysis

3. **New: Hiring Vetting Module** (To Build)
   ```
   tao/vetting/
   ├── __init__.py
   ├── candidate.py          # Candidate data model
   ├── assessment.py         # Assessment logic
   ├── benchmarks.py         # Benchmark database
   ├── reports.py            # Report generation
   ├── job_profiles.py       # Job-specific requirements
   └── dashboard.py          # Multi-candidate comparison
   ```

4. **New: Domain Library** (To Build)
   - 20-30 pre-built assessment domains
   - Each with curated patterns and documents
   - Novice → Expert question examples
   - Graded difficulty levels

5. **New: Assessment Portal** (To Build)
   - Knowledge assessment survey (MCQ)
   - Interest ranking interface
   - Candidate login/authentication
   - Progress dashboard (for candidates)

### Infrastructure Requirements

**Compute:**
- Docker containers (already using)
- 2-4 vCPU per candidate (for LLM calls)
- Scale: ~50 concurrent candidates = 100-200 vCPU

**Storage:**
- ~5MB per candidate (compressed query history)
- ~100 candidates = 500MB
- ~10,000 candidates/year = 50GB/year

**LLM API:**
- OpenAI/Anthropic API for query responses
- ~100-200 queries per candidate
- ~$2-5 per candidate in API costs
- Or: Self-hosted LLM (free, but requires GPU)

**Database:**
- PostgreSQL for candidate records, benchmarks
- Store: candidate_id, assigned_domains, timestamps, report_data
- Benchmark database: typical performance by role/seniority

**Security:**
- Candidate data encryption (at rest, in transit)
- GDPR compliance (data retention, right to deletion)
- SOC 2 Type II certification (for enterprise)
- Single sign-on (SSO) for enterprise customers

---

## External Requirements & Logistics

### Legal & Compliance

**1. Data Privacy**
- **GDPR** (EU): Right to access, deletion, portability
- **CCPA** (California): Similar rights
- **Data retention:** 1 year after hiring decision
- **Candidate consent:** Required before assessment
- **Data anonymization:** For benchmark aggregation

**2. Employment Law**
- **Adverse action:** Must explain rejection if requested
- **Bias auditing:** Regular fairness audits by third party
- **ATS integration:** Must integrate with Applicant Tracking Systems
- **EEOC compliance:** No discrimination by race, gender, age, etc.

**3. Terms of Service**
- Candidate agrees to assessment
- Company agrees to confidentiality
- BrainUse terms and pricing
- Service level agreements (SLA)

### Candidate Experience

**Before Assessment:**
- Clear communication: "This is a learning assessment, not a knowledge test"
- Time estimate: "10 days, ~1-2 hours per day at your pace"
- Privacy assurance: "Data used only for hiring decision, then deleted"
- Support: Chat/email for technical issues

**During Assessment:**
- No surveillance feel (no webcam, no screen recording)
- Natural interaction with ExFrame
- Progress indicator (optional: "You've completed 3/3 domains")
- Technical support available

**After Assessment:**
- Results available on request (within 7 days)
- Feedback provided if not hired (general, not detailed metrics)
- Option to delete data after 30 days

### Logistics & Operations

**1. Candidate Management**
- ATS integration (Greenhouse, Lever, Workday, etc.)
- Automated email workflows (invite → reminder → completion → report)
- Scheduling system (candidate picks 10-day window)
- Support ticketing (for technical issues)

**2. Domain Content Management**
- Domain library curator role
- Regular domain updates (quarterly)
- Quality assurance on patterns/documents
- Difficulty calibration testing

**3. Benchmark Database**
- Aggregate anonymized data across candidates
- Update benchmarks monthly
- Role-specific benchmarks (SWE, DevOps, Lead, etc.)
- Company-size benchmarks (startup vs. enterprise)

**4. Report Delivery**
- PDF generation (branded, confidential)
- Dashboard access for HR teams
- API for ATS integration
- Bulk export for analytics

**5. Customer Success**
- Onboarding for enterprise customers
- Training for hiring managers
- Best practices consultation
- Quarterly business reviews

---

## Implementation Phases

### Phase 1: MVP (3 months)

**Goal:** Prove concept with 10 pilot candidates

**Deliverables:**
- 3 assessment domains (Python, Cloud, Leadership)
- Basic vetting module (reports only)
- Manual candidate onboarding
- Simple PDF reports

**Validation:**
- Do reports predict interview performance?
- Do hiring managers find value?
- Are candidates satisfied with experience?

**Investment:** 1 engineer, 1 domain curator

### Phase 2: Beta (6 months)

**Goal:** 5 enterprise customers, 100 candidates

**Deliverables:**
- 10 assessment domains
- Automated onboarding workflow
- Dashboard for multi-candidate comparison
- Basic ATS integration (API)
- Benchmark database (100 candidates)

**Validation:**
- Customer retention (renew after 6 months?)
- Hiring quality improvement (track new hire performance)
- ROI for customers (cost savings vs. bad hires)

**Investment:** 2 engineers, 1 domain curator, 1 customer success

### Phase 3: Scale (12 months)

**Goal:** 50 customers, 5,000 candidates/year

**Deliverables:**
- 30 assessment domains
- Full ATS integrations (Greenhouse, Lever, etc.)
- Advanced analytics dashboard
- Benchmark database (5,000 candidates)
- Job profile marketplace
- SOC 2 Type II certification

**Validation:**
- Unit economics positive (CAC < LTV)
- Net revenue retention > 120%
- 10,000+ candidates assessed

**Investment:** 5 engineers, 2 domain curators, 3 customer success, 2 sales

### Phase 4: Enterprise (18-24 months)

**Goal:** Fortune 500 customers, 50,000 candidates/year

**Deliverables:**
- White-label solution
- Multi-language support
- Advanced bias auditing
- Predictive analytics (hiring outcome prediction)
- Custom domain building tools (enterprise can create own)

**Investment:** 15-20 person team

---

## Business Model

### Pricing Structure

**1. Per-Candidate Pricing** (SMB)
- $150 per candidate assessed
- Volume discounts (50+ candidates = $120, 100+ = $100)
- No monthly minimums
- Pay-as-you-go

**2. Subscription Pricing** (Mid-Market)
- $2,500/month for up to 20 candidates
- $5,000/month for up to 50 candidates
- $10,000/month for up to 100 candidates
- Overage: $100 per additional candidate

**3. Enterprise Pricing** (Fortune 500)
- Custom pricing based on volume
- Typical: $150,000-500,000/year
- Includes: White-label, custom domains, dedicated support
- Multi-year contracts

**4. Marketplace Revenue** (Future)
- Job profile templates: $500 per profile
- Custom domain creation: $2,000 per domain
- Benchmark reports: $50 per report

### Unit Economics

**Cost per Candidate Assessed:**
- LLM API: $3 (100 queries @ $0.03/query)
- Infrastructure: $2 (compute, storage)
- Support: $5 (amortized)
- **Total COGS: $10**

**At $150/candidate:**
- Gross margin: 93%
- Contribution margin: ~70% (after S&M, R&D allocation)

**Break-even:** ~300 candidates/month = $45K MRR

### Revenue Projections

**Year 1 (MVP + Beta):**
- 5 customers, 500 candidates
- Revenue: $75K
- Burn: $300K (1.5 eng, 0.5 curator, 0.5 CS)
- **Net: -$225K**

**Year 2 (Scale):**
- 50 customers, 5,000 candidates
- Revenue: $750K
- Burn: $1.2M (5 eng, 2 curator, 3 CS, 2 sales)
- **Net: -$450K**

**Year 3 (Enterprise):**
- 200 customers, 25,000 candidates
- Revenue: $3.75M
- Burn: $2.5M (15 eng, 5 ops, 10 GTM)
- **Net: +$1.25M (profitable)**

**Year 4 (Growth):**
- 500 customers, 75,000 candidates
- Revenue: $11.25M
- Burn: $5M
- **Net: +$6.25M**

### ROI for Customers

**Cost of Bad Hire:**
- IC Engineer: $100K (salary, recruiting, lost productivity)
- Senior Engineer: $150K
- Engineering Manager: $240K

**BrainUse Cost:**
- $150 per candidate
- Average: 5 candidates per hire = $750

**Value Proposition:**
- Reduce bad hire rate from 46% → 20% (target)
- Save 26% × $100K = $26,000 per hire
- ROI: 35x per hire

**Enterprise Example:**
- Hire 100 engineers/year
- Traditional: 46 bad hires × $100K = $4.6M lost
- With BrainUse: 20 bad hires × $100K = $2M lost
- **Savings: $2.6M/year**
- **BrainUse cost: $75K/year (500 candidates @ $150)**
- **ROI: 35x**

---

## Go-to-Market Strategy

### Target Customers (Year 1-2)

**Primary:** Tech companies with 50-500 employees
- High hiring volume (10-50 engineers/year)
- Budget for recruiting tools ($50K-200K/year)
- Pain: Bad hires costing $500K-2M/year
- Champions: VP Engineering, Head of Talent

**Secondary:** Tech consulting firms
- Hire constantly (100+ engineers/year)
- High churn tolerance (but expensive)
- Need objective assessment (not just interviews)

**Tertiary:** Enterprise HR tech buyers
- Pilot programs (10-20 candidates)
- Prove ROI before rollout
- Long sales cycles (6-12 months)

### Sales Strategy

**1. Product-Led Growth (PLG)**
- Free trial: Assess 3 candidates free
- Self-service signup
- Credit card required after trial
- Upsell to subscription

**2. Content Marketing**
- Blog: "The $200B Problem: Why Hiring is Broken"
- Whitepapers: "The Concert Pianist Principle"
- Webinars: "Evidence-Based Hiring"
- Case studies: Customer success stories

**3. Partnerships**
- ATS vendors (Greenhouse, Lever, Workday)
- Recruiting agencies (referral partnerships)
- HR communities (SHRM, ERE)

**4. Direct Sales (Enterprise)**
- Outbound to VP Eng / Head of Talent
- Pilot programs (10-20 candidates free)
- Proof of concept (3 months)
- Multi-year contracts

### Marketing Positioning

**Problem Statement:**
> "46% of new hires fail within 18 months, costing businesses $200B annually. Traditional hiring tools—resumes, interviews, references—measure credentials, not capability."

**Solution Statement:**
> "BrainUse reveals how candidates actually learn and think through 10 days of natural knowledge exploration. We don't ask what they know. We observe how their brains work."

**Key Differentiators:**
1. **Objective:** Data-driven vs. subjective interviews
2. **Predictive:** Learning ability predicts performance
3. **Natural:** 10-day observation vs. 1-hour pressure test
4. **Fair:** Measures capability, not credentials (no degree bias)
5. **Comprehensive:** Multiple dimensions (velocity, discipline, integration)

**Tagline:** *"Don't hire a resume. Hire a brain."*

---

## Success Metrics (KPIs)

### Product Metrics

**Candidate Experience:**
- Completion rate (target: >85%)
- Average time spent per day (target: 1-2 hours)
- Candidate satisfaction score (target: 4+/5)
- Technical issues per candidate (target: <0.1)

**Assessment Quality:**
- Report generation time (target: <1 hour)
- Classification accuracy (target: >90% agreement with human raters)
- Benchmark database size (target: 10,000+ candidates)

### Business Metrics

**Growth:**
- MRR growth rate (target: 20% month-over-month)
- Candidates assessed per month (target: 1,000 by end of Year 2)
- Customer count (target: 50 by end of Year 2)

**Retention:**
- Net revenue retention (target: >120%)
- Customer churn (target: <5% monthly)
- Logo retention (target: >90% annually)

**Unit Economics:**
- CAC (Customer Acquisition Cost): Target <$5,000
- LTV (Lifetime Value): Target >$50,000
- LTV:CAC ratio: Target >10:1
- Payback period: Target <6 months

### Outcome Metrics (Prove Value)

**Hiring Quality:**
- New hire success rate (target: improve from 54% to 80%)
- 90-day retention (target: >95%)
- Manager satisfaction with hire (target: 4.5+/5)

**Cost Savings:**
- Cost per quality hire (target: reduce 20%)
- Time to fill (target: reduce 15%)
- Recruiting team efficiency (target: +30% candidates per recruiter)

---

## Risks & Mitigations

### Technical Risks

**Risk:** LLM costs too high at scale
**Mitigation:** Self-hosted LLM option, API cost optimization, caching

**Risk:** Classification accuracy insufficient
**Mitigation:** Human validation loop, continuous model improvement, confidence thresholds

**Risk:** Candidates game the system
**Mitigation:** Gaming detection algorithms, red flag analysis, human review

### Business Risks

**Risk:** Customers don't see ROI
**Mitigation:** Outcome tracking, success metrics, customer success team

**Risk:** Slow enterprise sales cycles
**Mitigation:** PLG motion for SMB, pilot programs, land-and-expand

**Risk:** Regulatory issues (bias, discrimination)
**Mitigation:** Regular bias audits, legal review, fair hiring practices

### Market Risks

**Risk:** Competitors copy the approach
**Mitigation:** Patents, brand/network effects, benchmark database moat

**Risk:** Market not ready for evidence-based hiring
**Mitigation:** Education, thought leadership, early adopter focus

**Risk:** Integration challenges with ATS
**Mitigation:** API-first design, partnership with ATS vendors

---

## Conclusion

**BrainUse represents a paradigm shift in hiring:**

From:
- ❌ Resumes (credentials)
- ❌ Interviews (rehearsed answers)
- ❌ References (vague opinions)
- ❌ Tests (artificial pressure)

To:
- ✅ Natural learning observation
- ✅ Objective brain assessment
- ✅ Evidence-based decisions
- ✅ Predictive performance data

**The opportunity:**
- $200B problem (bad hires)
- Proven insight (concert pianist principle)
- Working technology (ExFrame + Tao)
- Clear path to market (PLG → SMB → Enterprise)

**Next steps:**
1. Build MVP (3 months)
2. Pilot with 10 candidates
3. Validate predictive power
4. Launch beta with 5 customers
5. Scale to $1M ARR in 18 months

**This isn't just a hiring tool. It's a new category: Brain Intelligence Platforms.**

---

**Document Version:** 1.0 (Draft)
**Last Updated:** 2026-02-16
**Authors:** ExFrame Team
**Status:** Ready for Review
